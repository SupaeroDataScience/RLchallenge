{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Easy\" league : handcrafted feature on state variables vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importation des modules nécessaires\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() #Utilisé pour regarder le temps écoulé lors de l'apprentissage\n",
    "\n",
    "############################## Définition des paramètres, modifiables #############################\n",
    "epochs = 10000 #Nombre de parties à jouer\n",
    "gamma = 0.8 # discount factor 0.8 \n",
    "alpha = 0.6 # learning rate 0.6\n",
    "prob = 0.9 #proba de jouer None lors d'un mouvement aléatoire\n",
    "epsilon = 0 # Initialisation d'epsilon (exploration/exploitation) #0 pour ne pas activer l'exploration aléatoire\n",
    "print_delay = 100 #Tous les combien de jeux on affiche un résumé et on enregistre le Q\n",
    "############################################## FIN ################################################\n",
    "\n",
    "\n",
    "############## Initialisation de Q (Décommenter une ligne, commenter l'autre) :####################\n",
    "Q = {} # pour reprendre un apprentissage de 0\n",
    "\n",
    "#Q = np.load('Q.npy').item() # pour reprendre l'apprentissage à partir d'un Q existant\n",
    "############################################ FIN ##################################################\n",
    "\n",
    "################ Définition des fonctions utilisées et initialisation des variables ###############\n",
    "\n",
    "#Les récompenses de sortie de p.act(action):\n",
    "reward_dict = { \"positive\": 1, \"negative\": 0.0, \"tick\": 0, \"loss\": -1000.0, \"win\": 0.0}\n",
    "\n",
    "#Fonction qui prend le jeu en entrée, et retourne une chaîne de caractère caractérisant l'état, de la forme hXXvYYsZZ\n",
    "#où XX représente l'écart horizontal entre l'oiseau et le prochain tuyau\n",
    "#YY est l'écart vertical entre l'oiseau et le haut du prochain tuyau\n",
    "#Les valeurs issues de l'état ont été divisées pour réduire le nombre d'états et améliorer l'apprentissage\n",
    "def getstate(jeu):\n",
    "    \n",
    "    #Acquisition des états sous forme de liste:\n",
    "    a = list(jeu.getGameState().values())\n",
    "    \n",
    "    #Ecart vertical entre l'oiseau et le haut du prochain tuyau, divisé par 10\n",
    "    #Le +512 a été ajouté pour ne pas avoir de valeurs négatives,\n",
    "    #ce qui n'as plus d'importance maintenant avec les chaines de\n",
    "    #caractères utilisées pour stocker les états\n",
    "    v_pos = np.int(np.round((a[0]-a[3]+512)/10))\n",
    "    \n",
    "    #Distance de l'oiseau au prochain tuyau (+20 idem précédemment) divisé par 10 également\n",
    "    h_pos = np.int(np.round((a[2]+20)/10))\n",
    "    \n",
    "    #Vitesse verticale de l'oiseau, divisée par 2\n",
    "    speed = np.int(np.round(a[1]/2))\n",
    "    \n",
    "    # Les divisions sont là pour ne pas avoir trop d'états possibles, et ainsi rendre l'apprentissage meilleur\n",
    "    #Création de la chaîne de caractères utilisée pour stocker les états dans le Q:\n",
    "    S = 'h'+np.str(h_pos)+'v'+np.str(v_pos)+'s'+np.str(speed)\n",
    "    \n",
    "    return S\n",
    "\n",
    "# Variables utilisées pour voir l'évolution\n",
    "scores = []\n",
    "average = []\n",
    "bests = []\n",
    "games = []\n",
    "    \n",
    "#Initilisation du jeu \"jeu\" et du joueur \"p\"\n",
    "jeu = FlappyBird()\n",
    "p = PLE(jeu, fps=30, frame_skip=1, num_steps=1, force_fps=True, display_screen=True, reward_values = reward_dict)\n",
    "################################################ FIN ##############################################\n",
    "\n",
    "\n",
    "########################################## Apprentissage ##########################################\n",
    "\n",
    "#Début des parties, on en joue un nombre : epochs\n",
    "for i in range(epochs):\n",
    "    \n",
    "    #Réinitialisation du jeu\n",
    "    p.reset_game()\n",
    "    \n",
    "    #Acquisition de l'état du jeu (s)\n",
    "    S = getstate(jeu)\n",
    "    \n",
    "    #Début de la boucle, tant que la partie n'est pas finie :\n",
    "    while(not jeu.game_over()):\n",
    "        \n",
    "        #Ajout de l'état dans Q si pas encore dedans\n",
    "        if S not in Q:\n",
    "            Q[S] = {None : 0, 119 : 0}\n",
    "            \n",
    "        #Choix de l'action à effectuer\n",
    "        if (np.random.random() < epsilon): # On choisit une action au hasard (Exploration)\n",
    "            if np.random.random() < prob: # proba prob de jouer l'action None\n",
    "                action = None\n",
    "            else:\n",
    "                action = 119\n",
    "        else: # On choisit la meilleure action, qui maximise Q(s,a) (Exploitation)\n",
    "            if Q[S][119] > Q[S][None]:\n",
    "                action = 119\n",
    "            else:\n",
    "                action = None\n",
    "\n",
    "        #On fait l'action \"action\" (a)\n",
    "        r = p.act(action)\n",
    "        \n",
    "        #On observe la récompense liée à cette action (r)\n",
    "        if r == 0: #Dans ce cas l'oiseau est encore en vie, on donne une récompense de 1\n",
    "            reward = 1\n",
    "        elif r == 1: #L'oiseau a franchi un tuyau, on donne une récompense de  100\n",
    "            reward = 100\n",
    "        else: #L'oiseau a touché un tuyau, le sol ou le plafond, on fait reward = r = -1000 (cf reward_dict)\n",
    "            reward = r\n",
    "        \n",
    "        # On observe le nouvel état new_S (s')\n",
    "        new_S = getstate(jeu)\n",
    "        \n",
    "        # Si le nouvel état n'est pas dans Q, on l'ajoute\n",
    "        if new_S not in Q:\n",
    "            Q[new_S] = {None : 0, 119 : 0}\n",
    "\n",
    "        # Recherche du max sur a' de Q(s',a')\n",
    "        maxQ = np.maximum(Q[new_S][None],Q[new_S][119])\n",
    "        \n",
    "        #Actualisation de Q(s,a)\n",
    "        Q[S][action] = Q[S][action] + alpha*(reward + gamma*maxQ - Q[S][action])      \n",
    "\n",
    "        #On actualise l'état courant\n",
    "        S = new_S\n",
    "        \n",
    "    #Après la fin de la partie, on ajoute le score final à la liste des scores\n",
    "    scores.append(jeu.getScore()-reward_dict[\"loss\"])\n",
    "    \n",
    "    # Mise à jour de la stratégie exploitation / exploration s'il y a lieu\n",
    "    if epsilon > 0.2:\n",
    "        epsilon -= (1.0/epochs)\n",
    "        \n",
    "    #Sauvegarde du Q courant, et affichage de quelques données pour suivre l'apprentissage\n",
    "    if ((i+1)%print_delay) == 0:\n",
    "        print(\"\")\n",
    "        print(\"Game #: %s\" % (i+1,))\n",
    "        print(\"Meilleur score sur les \",print_delay,\" derniers essais =\", np.max(scores))\n",
    "        print(\"Score moyen sur les \",print_delay,\" derniers essais =\", np.mean(scores))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        printime = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "        print(\"Temps écoulé : \", printime)\n",
    "        print('Taille du dictionnaire des états = ', len(Q))\n",
    "        average.append(np.mean(scores))\n",
    "        bests.append(np.max(scores))\n",
    "        games.append(i+1)\n",
    "        dire = './Q_learning_saves/Q_moyenne_'+np.str(np.mean(scores))\n",
    "        np.save(dire, Q)\n",
    "        np.save('Q',Q)\n",
    "        print('Save done avec moyenne: ' + np.str(np.mean(scores)))\n",
    "        scores = []\n",
    "\n",
    "########################################### FIN ###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des courbes des meilleurs scores et scores moyens lors des dernieres parties enregistrées\n",
    "plt.plot(games,average)\n",
    "plt.ylabel('Score moyen sur les 100 derniers essais')\n",
    "plt.show()\n",
    "plt.plot(games,bests)\n",
    "plt.ylabel('Meilleur score sur les 100 derniers essais')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Test du modèle\n",
    "## Utilisation du même script que \"run.py\" utilisé pour la notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "actions = [None, 119]\n",
    "Q = np.load('Q_good.npy').item()\n",
    "\n",
    "\n",
    "def getstate(state):\n",
    "    \n",
    "    a = list(state.values())\n",
    "    v_pos = np.int(np.round((a[0]-a[3]+512)/10)) # Différence de position verticale entre l'oiseau et le bas du prochain tuyau\n",
    "                          #+512 (hauteur totale en pixels) pour ne jamais être négatif\n",
    "    h_pos = np.int(np.round((a[2]+20)/10)) #Distance de l'oiseau au prochain tuyau\n",
    "    \n",
    "    speed = np.int(np.round(a[1]/2))\n",
    "    S = 'h'+np.str(h_pos)+'v'+np.str(v_pos)+'s'+np.str(speed)\n",
    "    return S\n",
    "\n",
    "def FlappyPolicy(state, screen):\n",
    "    \n",
    "    S = getstate(state)\n",
    "    if S not in Q:\n",
    "        action = None\n",
    "    elif Q[S][119]>Q[S][None]:\n",
    "        action = 119\n",
    "    else:\n",
    "        action = None\n",
    "        \n",
    "    return action\n",
    "\n",
    "game = FlappyBird(graphics=\"fixed\") # use \"fancy\" for full background, random bird color and random pipe color, use \"fixed\" (default) for black background and constant bird and pipe colors.\n",
    "p = PLE(game, fps=30, frame_skip=1, num_steps=1, force_fps=False, display_screen=True)\n",
    "# Note: if you want to see you agent act in real time, set force_fps to False. But don't use this setting for learning, just for display purposes.\n",
    "\n",
    "p.init()\n",
    "reward = 0.0\n",
    "\n",
    "nb_games = 100\n",
    "cumulated = np.zeros((nb_games))\n",
    "\n",
    "for i in range(nb_games):\n",
    "    p.reset_game()\n",
    "    \n",
    "    while(not p.game_over()):\n",
    "        state = game.getGameState()\n",
    "        screen = p.getScreenRGB()\n",
    "        action=FlappyPolicy(state, screen) ### Your job is to define this function.\n",
    "        \n",
    "        reward = p.act(action)\n",
    "        cumulated[i] = cumulated[i] + reward\n",
    "\n",
    "    print('Game #', i, ' score :',cumulated[i]+5)\n",
    "\n",
    "average_score = np.mean(cumulated+5)\n",
    "max_score = np.max(cumulated+5)\n",
    "\n",
    "print(\"Score moyen sur les 100 parties :\", average_score)\n",
    "print(\"Score max sur les 100 parties :\", max_score)\n",
    "\n",
    "if average_score > 15:\n",
    "    print(\"YOU WOOOOOOOOOOOOOOOOOOON !!!!!\")\n",
    "else:\n",
    "    print(\"Loser.\")\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
