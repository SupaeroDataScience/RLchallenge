{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import RMSprop, sgd\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Game #: 100\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.03\n",
      "Temps écoulé :  00:00:18\n",
      "Save done avec moyenne: 0.03\n",
      "\n",
      "Game #: 200\n",
      "Meilleur score sur les  100  derniers essais = 2.0\n",
      "Score moyen sur les  100  derniers essais = 0.07\n",
      "Temps écoulé :  00:00:38\n",
      "Save done avec moyenne: 0.07\n",
      "\n",
      "Game #: 300\n",
      "Meilleur score sur les  100  derniers essais = 4.0\n",
      "Score moyen sur les  100  derniers essais = 0.1\n",
      "Temps écoulé :  00:01:02\n",
      "Save done avec moyenne: 0.1\n",
      "\n",
      "Game #: 400\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.06\n",
      "Temps écoulé :  00:01:20\n",
      "Save done avec moyenne: 0.06\n",
      "\n",
      "Game #: 500\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.05\n",
      "Temps écoulé :  00:01:39\n",
      "Save done avec moyenne: 0.05\n",
      "\n",
      "Game #: 600\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.02\n",
      "Temps écoulé :  00:01:59\n",
      "Save done avec moyenne: 0.02\n",
      "\n",
      "Game #: 700\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.1\n",
      "Temps écoulé :  00:02:20\n",
      "Save done avec moyenne: 0.1\n",
      "\n",
      "Game #: 800\n",
      "Meilleur score sur les  100  derniers essais = 2.0\n",
      "Score moyen sur les  100  derniers essais = 0.06\n",
      "Temps écoulé :  00:02:39\n",
      "Save done avec moyenne: 0.06\n",
      "\n",
      "Game #: 900\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.06\n",
      "Temps écoulé :  00:02:58\n",
      "Save done avec moyenne: 0.06\n",
      "\n",
      "Game #: 1000\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.07\n",
      "Temps écoulé :  00:03:18\n",
      "Save done avec moyenne: 0.07\n",
      "\n",
      "Game #: 1100\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.07\n",
      "Temps écoulé :  00:03:37\n",
      "Save done avec moyenne: 0.07\n",
      "\n",
      "Game #: 1200\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.09\n",
      "Temps écoulé :  00:03:57\n",
      "Save done avec moyenne: 0.09\n",
      "\n",
      "Game #: 1300\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.05\n",
      "Temps écoulé :  00:04:14\n",
      "Save done avec moyenne: 0.05\n",
      "\n",
      "Game #: 1400\n",
      "Meilleur score sur les  100  derniers essais = 2.0\n",
      "Score moyen sur les  100  derniers essais = 0.09\n",
      "Temps écoulé :  00:04:33\n",
      "Save done avec moyenne: 0.09\n",
      "\n",
      "Game #: 1500\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.09\n",
      "Temps écoulé :  00:04:52\n",
      "Save done avec moyenne: 0.09\n",
      "\n",
      "Game #: 1600\n",
      "Meilleur score sur les  100  derniers essais = 1.0\n",
      "Score moyen sur les  100  derniers essais = 0.04\n",
      "Temps écoulé :  00:05:09\n",
      "Save done avec moyenne: 0.04\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldeschamps/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() #Utilisé pour regarder le temps écoulé lors de l'apprentissage\n",
    "\n",
    "\n",
    "# Définition des paramètres, modifiables\n",
    "epochs = 5000 #Nombre de parties à jouer\n",
    "gamma = 0.8 # discount factor 0.99\n",
    "alpha=0.6 # learning rate 0.1\n",
    "prob = 0.8 #proba de jouer None lors d'un mouvement aléatoire\n",
    "experience_replay = False # Activer l'experience replay ? sûrement nécessaire pour meilleur apprentissage\n",
    "batchSize = 32 # Taille du mini batch\n",
    "buffer = 5000 #Taille du buffer\n",
    "print_delay = 100\n",
    "\n",
    "# FAIRE ATTENTION AUX RECOMPENSES : Jouer peut être sur des récompenses à posteriori plutôt que ce dict\n",
    "reward_dict = { \"positive\": 1, \"negative\": 0.0, \"tick\": 0, \"loss\": -1000.0, \"win\": 0.0} #values\n",
    "\n",
    "\n",
    "# Définition des fonctions utilisées et initialisation des variables\n",
    "\n",
    "def getstate(jeu): #On ne prend que les éléments 0,1,2,3 : vitesse, pos verticale et horizontale de l'oiseau\n",
    "                                                          #et pos verticale du haut du prochain tuyau\n",
    "    return list(jeu.getGameState().values())[0:4]\n",
    "\n",
    "actions = [None, 119]\n",
    "update=0\n",
    "h=0 #Taille du vecteur buffer, initialisée à 0\n",
    "replay = [] # vecteur de buffer initialisé vide\n",
    "epsilon = 0 # Initialisation d'epsilon (exploration/exploitation)\n",
    "score = 0 # Variable utilisée pour mémoriser le meilleur score obtenu sur les epochs parties\n",
    "time_for_save = datetime.now().strftime('%d-%m-%Y-%H:%M:%S') # Pour l'enregistrement\n",
    "path = os.path.join('log-files', time_for_save)\n",
    "os.makedirs(path) # Création du dossier\n",
    "filenames = glob.glob('*.ipynb') # Sauvegarde du fichier notebook utilisé pour générer le réseau\n",
    "for filename in filenames:\n",
    "    shutil.copy(filename, path)\n",
    "    \n",
    "# Variables utilisées pour voir l'évolution\n",
    "scores = []\n",
    "average = []\n",
    "bests = []\n",
    "games = []\n",
    "\n",
    "#Initilisation du jeu \"jeu\" et du joueur \"p\"\n",
    "jeu = FlappyBird()\n",
    "\n",
    "#frame_skip = The number of times we skip getting observations while repeat an action\n",
    "#num_steps = The number of times we repeat an action.\n",
    "#force_fps = True pour ne pas jouer en temps réel (plus vite)\n",
    "#display_screen = True pour afficher le déroulement de la partie en direct\n",
    "#reward_values = This contains the rewards we wish to set give our agent\n",
    "\n",
    "p = PLE(jeu, fps=30, frame_skip=1, num_steps=1, force_fps=True, display_screen=True, reward_values = reward_dict)\n",
    "\n",
    "p.reset_game()\n",
    "\n",
    "\n",
    "# Création du réseau de neurones\n",
    "model = Sequential()\n",
    "# # Première couche, 150 avec 8 inputs, les 8 composantes du vecteur d'état\n",
    "# model.add(Dense(150, kernel_initializer='lecun_uniform', input_shape=(8,)))\n",
    "# Première couche, 150 avec 4 inputs, les 4 composantes du vecteur d'état qu'on a choisis\n",
    "model.add(Dense(150, kernel_initializer='lecun_uniform', input_shape=(4,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "# # Couche cachée, taille 300\n",
    "# model.add(Dense(300, kernel_initializer='lecun_uniform'))\n",
    "# model.add(Activation('relu'))\n",
    "# Couche cachée, taille 150\n",
    "model.add(Dense(150, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('relu'))\n",
    "# Couche de sortie, taille 2 comme les 2 actions\n",
    "model.add(Dense(2, kernel_initializer='lecun_uniform'))\n",
    "model.add(Activation('linear'))\n",
    "# Compilation du modèle\n",
    "model.compile(loss='mse', optimizer=\"rmsprop\")\n",
    "\n",
    "\n",
    "\n",
    "#Début des parties, on en joue un nombre : epochs\n",
    "for i in range(epochs):\n",
    "    #Réinitialisation du jeu\n",
    "    p.reset_game()\n",
    "    #Acquisition de l'état du jeu (s)\n",
    "    state = getstate(jeu)\n",
    "    \n",
    "    #Début de la boucle, tant que la partie n'est pas finie :\n",
    "    while(not jeu.game_over()):\n",
    "        \n",
    "        # Evaluation de la valeur de la policy pour les deux actions : Q(s,a)\n",
    "        qval = model.predict(np.array(state).reshape(1,len(state)), batch_size=batchSize)\n",
    " \n",
    "        #Stratégie Exploration/exploitation \n",
    "        if (np.random.random() < epsilon): # On choisit une action au hasard, proba prob pour l'action None\n",
    "            if np.random.random() < prob:\n",
    "                action = actions[0]\n",
    "            else:\n",
    "                action = actions[1]\n",
    "        else: # On choisit la meilleure action, qui maximise qval (Q(s,a))\n",
    "            if qval[0][0] > qval[0][1]:\n",
    "                action = actions[0]\n",
    "            else:\n",
    "                action = actions[1]\n",
    "\n",
    "        #On fait l'action \"action\" (a), on observe le nouvel état new_state (s') et la récompense reward (r)\n",
    "        r = p.act(action)\n",
    "        \n",
    "        #On observe la récompense liée à cette action\n",
    "        if r == 0: #Dans ce cas l'oiseau est encore en vie, on donne une récompense de 1\n",
    "            reward = 1\n",
    "        elif r == 1: #L'oiseau a franchi un tuyau, on donne une récompense de  100\n",
    "            reward = 100\n",
    "        else: #L'oiseau a touché un tuyau, le sol ou le plafond, on fait reward = r = -1000 (cf reward_dict)\n",
    "            reward = r\n",
    "            \n",
    "        new_state = getstate(jeu)   \n",
    "        \n",
    "        #Actualisation du score si meilleur que le précédent\n",
    "        if jeu.getScore() > score :\n",
    "            score = jeu.getScore()\n",
    "            \n",
    "        if not experience_replay:# si on n'a pas activé la mémoire\n",
    "            #Calcul des valeurs Q(s',a) POURQUOI BATCH SIZE ????\n",
    "            newQ = model.predict(np.array(new_state).reshape(1,len(state)), batch_size=batchSize)\n",
    "            # Recherche du max\n",
    "            maxQ = np.max(newQ)\n",
    "            #Initialisation de y à Q(s,a)\n",
    "            y = np.zeros((1,2))\n",
    "            y[:] = qval[:]\n",
    "            \n",
    "            #Calcul de l'update\n",
    "            if reward != reward_dict[\"loss\"]: # Partie pas perdue\n",
    "                update = (reward + gamma * maxQ)\n",
    "            else:\n",
    "                update = reward\n",
    "                \n",
    "            # On met la valeur \"update\" dans la composante de y qui correspond à l'action jouée\n",
    "            if action == None:\n",
    "                y[0][0] = update #target output\n",
    "            else :\n",
    "                y[0][1] = update\n",
    "                \n",
    "            #On entraine le réseau de neurones avec state en entrée, y en sortie\n",
    "            model.fit(np.array(state).reshape(1, len(state)), y, batch_size=batchSize, epochs=1, verbose=0)\n",
    "            \n",
    "            #On actualise l'état\n",
    "            state = new_state\n",
    "            \n",
    "            #clear_output(wait=True)\n",
    "            \n",
    "        else: #Si on a activé la mémoire\n",
    "            \n",
    "            if (len(replay) < buffer): #Si le buffer n'est pas plein, on ajoute le dernier sars\n",
    "                replay.append((state, action, reward, new_state))\n",
    "            else: #Si il est plein, on remplace une vieille valeur et fait de l'apprentissage\n",
    "                \n",
    "                if (h < (buffer-1)): #Si h est plus petit que la taille du vecteur replay :\n",
    "                    h += 1 #On incrémente h pour changer la composante suivante du vecteur replay\n",
    "                else: #Sinon (donc si h pointe vers le dernier élément du replay)\n",
    "                    h = 0 # On remet h à 0\n",
    "                # On remplace l'élément h par le sars nouvellement acquis\n",
    "                replay[h] = (state, action, reward, new_state)\n",
    "                \n",
    "                #On prends batchSize éléments aléatoirement dans notre replay memory :\n",
    "                minibatch = random.sample(replay, batchSize)\n",
    "                \n",
    "                # Réinitialisation de X_train et y_train\n",
    "                X_train = []\n",
    "                y_train = []\n",
    "                \n",
    "                #On prend les éléments dans minibatch, le but est de remplir X_train et y_train\n",
    "                for memory in minibatch: #On parcourt les éléments sélectionnés\n",
    "                    #On cherche le max de Q(s',a) :\n",
    "                    #On stocke s,a,r,s pour chaque élément\n",
    "                    old_state, action, reward, new_state = memory\n",
    "                    #On calcule la q_value pour l'ancien état s Q(s,.)\n",
    "                    old_qval = model.predict(np.array(old_state).reshape(1,len(old_state)), batch_size=1)\n",
    "                    #On calcule la q_value pour le nouvel état s' Q(s',.)\n",
    "                    newQ = model.predict(np.array(new_state).reshape(1,len(new_state)), batch_size=1)\n",
    "                    #On stocke la valeur maximale de la q_value du nouvel état max(a)Q(s',a)\n",
    "                    maxQ = np.max(newQ)\n",
    "                    # Initialisation de y aux valeurs de Q(s,.) de l'ancien état\n",
    "                    y = np.zeros((1,2))\n",
    "                    y[:] = old_qval[:]\n",
    "                    \n",
    "                    #Calcul de l'update en fonction de si l'état est terminal ou non\n",
    "                    if reward != reward_dict[\"loss\"]: #Etat non terminal\n",
    "                        update = (reward + (gamma * maxQ))\n",
    "                    else: #Etat terminal\n",
    "                        update = reward\n",
    "                    # On met la valeur \"update\" dans la composante de y qui correspond à l'action jouée\n",
    "                    if action == None:\n",
    "                        y[0][0] = update\n",
    "                    else :\n",
    "                        y[0][1] = update\n",
    "                        \n",
    "                    #On ajoute old_state et y à X_train et y_train\n",
    "                    X_train.append(np.array(old_state).reshape(len(old_state),))\n",
    "                    y_train.append(np.array(y).reshape(2,))\n",
    "                    \n",
    "                #On met X_train et y_train sous forme de tableau\n",
    "                X_train = np.array(X_train)\n",
    "                y_train = np.array(y_train)\n",
    "                \n",
    "                #Entrainement du réseau avec batchSize éléments\n",
    "                model.fit(X_train, y_train, batch_size=batchSize, epochs=1, verbose=0)\n",
    "                \n",
    "                #Mise à jour de l'état avec le nouvel état\n",
    "                state = new_state\n",
    "            #clear_output(wait=True)\n",
    "    \n",
    "    #Après la fin de la partie, on ajoute le score final à la liste des scores\n",
    "    scores.append(jeu.getScore()-reward_dict[\"loss\"])\n",
    "    \n",
    "    # Mise à jour de la stratégie exploitation / exploration\n",
    "    if epsilon > 0.2:\n",
    "        epsilon -= (1.0/epochs)\n",
    "    \n",
    "        \n",
    "    #Sauvegarde du Q courant, et affichage de quelques donnée pour suivre l'apprentissage\n",
    "    if ((i+1)%print_delay) == 0:\n",
    "        print(\"\")\n",
    "        print(\"Game #: %s\" % (i+1,))\n",
    "        print(\"Meilleur score sur les \",print_delay,\" derniers essais =\", np.max(scores))\n",
    "        print(\"Score moyen sur les \",print_delay,\" derniers essais =\", np.mean(scores))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        printime = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    "        print(\"Temps écoulé : \", printime)\n",
    "        average.append(np.mean(scores))\n",
    "        bests.append(np.max(scores))\n",
    "        games.append(i+1)\n",
    "        print('Save done avec moyenne: ' + np.str(np.mean(scores)))\n",
    "        scores = []\n",
    "        now = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "        name = 'FlappyBird_'+now+'.dqf'\n",
    "        dire = './log-files/'+time_for_save+'/'+name\n",
    "        model.save(dire)\n",
    "            \n",
    "print(\"Best score =\", score)\n",
    "dire = './log-files/'+time_for_save+'/FlappyBird.dqf'\n",
    "model.save(dire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test du réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "actions = [None, 119]\n",
    "model = load_model('FlappyBird.dqf')\n",
    "\n",
    "def FlappyPolicy(state, screen):\n",
    "\tstate_list = list(state.values())[0:4]\n",
    "\t\n",
    "\tqval = model.predict(np.array(state_list).reshape(1,len(state_list)), batch_size=1)\n",
    "\tif qval[0][0] > qval[0][1]:\n",
    "\t    action = actions[0]\n",
    "\telse:\n",
    "\t    action = actions[1]\n",
    "\tprint(action)\n",
    "\treturn action\n",
    "\n",
    "\n",
    "game = FlappyBird()\n",
    "p = PLE(game, fps=30, frame_skip=1, num_steps=1, force_fps=True, display_screen=False)\n",
    "\n",
    "p.init()\n",
    "reward = 0.0\n",
    "\n",
    "nb_games = 100\n",
    "cumulated = np.zeros((nb_games))\n",
    "\n",
    "for i in range(nb_games):\n",
    "    p.reset_game()\n",
    "    \n",
    "    while(not p.game_over()):\n",
    "        state = game.getGameState()\n",
    "        screen = p.getScreenRGB()\n",
    "        action=FlappyPolicy(state, screen) ### Your job is to define this function.\n",
    "        \n",
    "        reward = p.act(action)\n",
    "        cumulated[i] = cumulated[i] + reward\n",
    "    print('Game #', i, ' score :',cumulated[i]+5)\n",
    "\n",
    "average_score = np.mean(cumulated+5)\n",
    "max_score = np.max(cumulated+5)\n",
    "\n",
    "print(\"Score moyen sur les 100 parties :\", average_score)\n",
    "print(\"Score max sur les 100 parties :\", max_score)\n",
    "\n",
    "if average_score > 15:\n",
    "    print(\"YOU WOOOOOOOOOOOOOOOOOOON !!!!!\")\n",
    "else:\n",
    "    print(\"Loser.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
