{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Flatten, Dropout\n",
    "from keras import initializers, optimizers\n",
    "\n",
    "from PIL import Image\n",
    "np.set_printoptions(precision=3)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(precision=3)\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn = load_model('temp/flappy-bucket-2/nickel600000.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confirm network reinitialisation ?1\n",
      "network re-initialized\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    confirmation = int(input(\"confirm network reinitialisation ?\")) \n",
    "except:\n",
    "    print(\"failed\")\n",
    "    confirmation = 0\n",
    "if(confirmation == 1):\n",
    "    dqn = Sequential()\n",
    "    #1st layer\n",
    "    dqn.add(Conv2D(filters=16, kernel_size=(8,8), strides=4, activation=\"relu\", input_shape=(67,104,4)))\n",
    "    #2nd layer\n",
    "    dqn.add(Conv2D(filters=32, kernel_size=(4,4), strides=2, activation=\"relu\"))\n",
    "    dqn.add(Flatten())\n",
    "    #3rd layer\n",
    "    dqn.add(Dense(units=256, activation=\"relu\"))\n",
    "    #output layer\n",
    "    dqn.add(Dense(units=2, activation=\"linear\"))\n",
    "    \n",
    "    my_adam = optimizers.Adam(lr=0.00001)\n",
    "    dqn.compile(optimizer=my_adam, loss=\"mean_squared_error\")\n",
    "    #dqn.compile(optimizer=\"rmsprop\", loss=\"mean_squared_error\")\n",
    "    print(\"network re-initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------------functions--------------------------\n",
    "\n",
    "\n",
    "def BasicPolicy(state):\n",
    "    if(state[\"player_y\"] > (state[\"next_pipe_bottom_y\"] + state[\"next_pipe_top_y\"])/2):\n",
    "        return(1)\n",
    "    else:\n",
    "        return(0)\n",
    "    \n",
    "def epsilon(step):\n",
    "    final = 0.1\n",
    "    fraction = 1/3\n",
    "    if step < total_steps*fraction:\n",
    "        return 1-(step)*(1-final)/(total_steps*fraction)\n",
    "    return final\n",
    "\n",
    "def clip_reward_2(r, game_over):\n",
    "    if game_over:\n",
    "        return(-1)\n",
    "    if r ==1:\n",
    "        return(1)\n",
    "    return(0.1)\n",
    "    #if ((state[\"player_y\"] - (state[\"next_pipe_bottom_y\"] + state[\"next_pipe_top_y\"])/2) !=0):\n",
    "    #    return(steps/500/np.sqrt(np.absolute(state[\"player_y\"] - (state[\"next_pipe_bottom_y\"] + state[\"next_pipe_top_y\"])/2)))\n",
    "    \n",
    "    \n",
    "\n",
    "def greedy_action(network, x):\n",
    "    Q = network.predict(np.array([x]))\n",
    "    return np.argmax(Q)\n",
    "\n",
    "def process_screen(x):\n",
    "    h = 130\n",
    "    w = 80\n",
    "    return 256*resize(rgb2gray(x), (w,h))[int(w/6):,0:int(4/5*h)]\n",
    "\n",
    "def int_to_action(n):\n",
    "    if n == 0:\n",
    "        return(None)\n",
    "    if n == 1:\n",
    "        return(119)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------Memory Buffer-----------------------------------\n",
    "class MemoryBuffer:\n",
    "    \"An experience replay buffer using numpy arrays\"\n",
    "    def __init__(self, length, screen_shape, action_shape):\n",
    "        self.length = length\n",
    "        self.screen_shape = screen_shape\n",
    "        self.action_shape = action_shape\n",
    "        shape = (length,) + screen_shape\n",
    "        self.screens_x = np.zeros(shape, dtype=np.uint8) # starting states\n",
    "        self.screens_y = np.zeros(shape, dtype=np.uint8) # resulting states\n",
    "        shape = (length,) + action_shape\n",
    "        self.actions = np.zeros(shape, dtype=np.uint8) # actions\n",
    "        self.rewards = np.zeros((length,1), dtype=np.float64) # rewards\n",
    "        self.terminals = np.zeros((length,1), dtype=np.bool) # true if resulting state is terminal\n",
    "        self.terminals[-1] = True\n",
    "        self.index = 0 # points one position past the last inserted element\n",
    "        self.size = 0 # current size of the buffer\n",
    "    \n",
    "    def append(self, screenx, a, r, screeny, d):\n",
    "        self.screens_x[self.index] = screenx\n",
    "        #plt.imshow(screenx)\n",
    "        #plt.show()\n",
    "        #plt.imshow(self.screens_x[self.index])\n",
    "        #plt.show()\n",
    "        self.actions[self.index] = a\n",
    "        self.rewards[self.index] = r\n",
    "        self.screens_y[self.index] = screeny\n",
    "        self.terminals[self.index] = d\n",
    "        self.index = (self.index+1) % self.length\n",
    "        self.size = np.min([self.size+1,self.length])\n",
    "    \n",
    "    def stacked_frames_x(self, index):\n",
    "        im_deque = deque(maxlen=4)\n",
    "        pos = index % self.length\n",
    "        for i in range(4): # todo\n",
    "            im = self.screens_x[pos]\n",
    "            im_deque.appendleft(im)\n",
    "            test_pos = (pos-1) % self.length\n",
    "            if self.terminals[test_pos] == False:\n",
    "                pos = test_pos\n",
    "        return np.stack(im_deque, axis=-1)\n",
    "    \n",
    "    def stacked_frames_y(self, index):\n",
    "        im_deque = deque(maxlen=4)\n",
    "        pos = index % self.length\n",
    "        for i in range(4): # todo\n",
    "            im = self.screens_y[pos]\n",
    "            im_deque.appendleft(im)\n",
    "            test_pos = (pos-1) % self.length\n",
    "            if self.terminals[test_pos] == False:\n",
    "                pos = test_pos\n",
    "        return np.stack(im_deque, axis=-1)\n",
    "    \n",
    "    def minibatch(self, size):\n",
    "        #return np.random.choice(self.data[:self.size], size=sz, replace=False)\n",
    "        indices = np.random.choice(self.size, size=size, replace=False)\n",
    "        x = np.zeros((size,)+self.screen_shape+(4,))\n",
    "        y = np.zeros((size,)+self.screen_shape+(4,))\n",
    "        for i in range(size):\n",
    "            x[i] = self.stacked_frames_x(indices[i])\n",
    "            y[i] = self.stacked_frames_y(indices[i])\n",
    "        return x, self.actions[indices], self.rewards[indices], y, self.terminals[indices]\n",
    "\n",
    "\n",
    "mini_batch_size = 32\n",
    "replay_memory_size = 100000\n",
    "replay_memory = MemoryBuffer(replay_memory_size, (67,104), (1,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\logiciels\\Anaconda\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:138061\r"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\logiciels\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------Training algorithm-------------------------------\n",
    "game = FlappyBird(graphics=\"fixed\") # use \"fancy\" for full background, random bird color and random pipe color, use \"fixed\" (default) for black background and constant bird and pipe colors.\n",
    "p = PLE(game, fps=30, frame_skip=1, num_steps=1, force_fps=True, display_screen=False)\n",
    "# Note: if you want to see you agent act in real time, set force_fps to False. But don't use this setting for learning, just for display purposes.\n",
    "\n",
    "evaluation_period = 5000\n",
    "total_steps = 300000\n",
    "gamma = 0.99\n",
    "\n",
    "nb_epochs = total_steps // evaluation_period\n",
    "epoch=-1\n",
    "scoreQ = np.zeros((nb_epochs))\n",
    "\n",
    "p.init()\n",
    "reward = 0.0\n",
    "cumulated = np.zeros((total_steps))\n",
    "i = 0\n",
    "\n",
    "p.reset_game()\n",
    "screen_x = process_screen(p.getScreenRGB())\n",
    "stacked_x = deque([screen_x, screen_x, screen_x, screen_x], maxlen=4)\n",
    "x = np.stack(stacked_x, axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for step in range(total_steps):\n",
    "\n",
    "    # action selection\n",
    "    a_predict = greedy_action(dqn, x)\n",
    "    if np.random.rand() < 0.1:\n",
    "        if np.random.randint(0,10) < 2:\n",
    "            a = 1\n",
    "        else:\n",
    "            a = 0\n",
    "    else:\n",
    "        a = a_predict\n",
    "    action = int_to_action(a)\n",
    "    \n",
    "    # step\n",
    "    reward_raw = p.act(action)\n",
    "    reward = clip_reward_2(reward_raw, p.game_over())\n",
    "    screen_y = process_screen(p.getScreenRGB())\n",
    "    replay_memory.append(screen_x, a, reward, screen_y, p.game_over())\n",
    "    \n",
    "    \n",
    "    # train\n",
    "    if step>mini_batch_size:\n",
    "        X,A,R,Y,D = replay_memory.minibatch(mini_batch_size)\n",
    "        QY = dqn.predict(Y)\n",
    "        QYmax = QY.max(1).reshape((mini_batch_size,1))\n",
    "        update = R + gamma * (1-D) * QYmax\n",
    "        QX = dqn.predict(X)\n",
    "        QX[np.arange(mini_batch_size), A.ravel()] = update.ravel()\n",
    "        dqn.train_on_batch(x=X, y=QX)\n",
    "        \n",
    "    #next transition \n",
    "    if(p.game_over()):\n",
    "        # restart episode\n",
    "        p.reset_game()\n",
    "        screen_x = process_screen(p.getScreenRGB())\n",
    "        stacked_x = deque([screen_x, screen_x, screen_x, screen_x], maxlen=4)\n",
    "        x = np.stack(stacked_x, axis=-1)\n",
    "    else: \n",
    "        # keep going\n",
    "        screen_x = screen_y\n",
    "        stacked_x.append(screen_x)\n",
    "        x = np.stack(stacked_x, axis=-1)\n",
    "        \n",
    "    print(\"step:{}\".format(step), end='\\r')\n",
    "    \n",
    "dqn.save(\"temp/flappy-bucket-2/300000.h5\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\logiciels\\Anaconda\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\logiciels\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------Evaluation algorithm-------------------------------\n",
    "game = FlappyBird(graphics=\"fixed\") # use \"fancy\" for full background, random bird color and random pipe color, use \"fixed\" (default) for black background and constant bird and pipe colors.\n",
    "p = PLE(game, fps=30, frame_skip=1, num_steps=1, force_fps=False, display_screen=True)\n",
    "# Note: if you want to see you agent act in real time, set force_fps to False. But don't use this setting for learning, just for display purposes.\n",
    "trials = 100\n",
    "scores = np.zeros((trials))\n",
    "\n",
    "for i in range(trials):\n",
    "    p.reset_game()\n",
    "    screen_x = process_screen(p.getScreenRGB())\n",
    "    state = game.getGameState()\n",
    "    stacked_x = deque([screen_x, screen_x, screen_x, screen_x], maxlen=4)\n",
    "    x = np.stack(stacked_x, axis=-1)\n",
    "    while p.game_over()!=True:            \n",
    "        #action selection\n",
    "        action = int_to_action(greedy_action(dqn, x))\n",
    "\n",
    "        reward_raw = p.act(action)       \n",
    "        screen_y = process_screen(p.getScreenRGB())\n",
    "        scores[i] = scores[i] + reward_raw\n",
    "        \n",
    "        #next state\n",
    "        screen_x = screen_y\n",
    "        stacked_x.append(screen_x)\n",
    "        x = np.stack(stacked_x, axis=-1)\n",
    "    print(scores[i])\n",
    "        \n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confirm network save ?1\n",
      "file name ?nickel600000\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Save dqn--------------------------\n",
    "try:\n",
    "    confirmation = int(input(\"confirm network save ?\")) \n",
    "except:\n",
    "    print(\"failed\")\n",
    "    confirmation = 0\n",
    "if(confirmation == 1):\n",
    "    name = str(input(\"file name ?\")) \n",
    "    dqn.save(\"temp/flappy-bucket-2/\" + name + '.h5')\n",
    "else:\n",
    "    print(\"aborted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
