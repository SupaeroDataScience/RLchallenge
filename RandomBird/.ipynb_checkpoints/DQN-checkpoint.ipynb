{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import numpy as np\n",
    "from FlappyAgent import FlappyPolicy\n",
    "\n",
    "def FlappyPolicy(state, screen):\n",
    "    action=None\n",
    "    if(np.random.randint(0,2)<1):\n",
    "        action=119\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Q-learning theory :\n",
    "\n",
    "1. choose an action for the state s (glie)\n",
    "2. update the value of Q(s,glie(s)) with r and Q(s',ac) : y_i = r + γ*Q_theta_i-1(s',ac) \n",
    "(network used to obtain the maximum argmax ac Q_theta_i-1(s',ac))\n",
    "3. update the value of the weights so that Q_theta_i(s,glie(s)) matches the new target y_i provided by step 2 (network trained) as much as possible, by reducing : cost_function(theta_i) through the gradient and backprog...\n",
    "\n",
    "Notice the difference between step 2 and 3. In step 2, the network is used, as opposed to being trained as in step 3.\n",
    "Notice that in step 2, when updating, there is no reference to the previous value of Q for the same state-space couple. That is due to the fact that this information is supposed to be more or less contained in the weights of the neural network at this time (our Q is learning). \n",
    "We could say that all the information we get from our samples (s,a,r,s') in step 2 are incorporated/saved in our neural network's weights and biases in step 3. However, that raises the question of catastrophic forgetting : updating the value of the weights samples by samples is inneficient. We risk forgetting about what was learned previously in some region of the state-action space, if we flood the Q-network with too many samples from another region because sequences of states are often correlated and in a region (ie poids efficace pour une region etat espace, mais pas tout du tout adapte pour dautres regions espaces etats. Cela est problematique dans la step 2 car on utilise le reseau pour evaluer le maximum amax :amax = argmax ac Q(s',ac) avec des Q(s',ac) qui ne sont pas fideles a la realite). \n",
    "Experience replay can counteract that trend by feeding the network with training data from old experiences so that it does not forget how to behave there (It is as if I was living in China for 5 years, exposed to the Chinese language everywhere, but still listen to English, French and Portuguese podcast so I do not forget it completely).\n",
    "\n",
    "\n",
    "Input layer of the deep Q : one input = one state variable\n",
    "Output layer of the deep Q : one neuron = one action (the deep Q-network) (results in less forward pass -> one forward pass gives the Qvalues for all action for the same state). Which mean that the deep Q network does not take the glie action into its input layer, but rather the four last actions associated to the four screens in its history.\n",
    "\n",
    "The advantage of deep Q learning is that there is no feature selection (we can give the nework the whole screen in our case, just like a human), and he will figure out on his own patterns to best fit with the target. This generalization of how to react to certain types of states make it useful to deal with big state spaces.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep theory in practice :\n",
    "   \n",
    "We need only the following : video input, the reward and terminal signals, and the set of possible actions (just like a human would).\n",
    "To understand the situation from one image is not enough (no access to speed for instance).\n",
    "So if we treat four frames of 20x20 pixels (for instance), we have 20x20x4 + 4 (or 3?) inputs in the input layer (+4 (or 3?) bc we have to choose an action for each frame) i.e our states are sequences.\n",
    "\n",
    "*********************************************\n",
    "Do not forget if you want to reset the rewards and explain why. In atari :\n",
    "While we evaluatedouragentsontherealandunmodiﬁedgames,wemadeonechangetotherewardstructure of the games during training only. Since the scale of scores varies greatly from game to game, we ﬁxed all positive rewards to be 1 and all negative rewards to be−1, leaving 0 rewards unchanged. Clipping the rewards in this manner limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. At the same time, it could affect the performance of our agent since it cannot differentiate between rewards of different magnitude.\n",
    "***********************************************\n",
    "The mask trick for terminal state to understand.\n",
    "\n",
    "\n",
    "\n",
    "***************************************\n",
    "Implementing experience replay :\n",
    "\n",
    "We store the agent’s experiences at each time-step, et = (st,at,rt,st+1) in a data-set D = e1,...,eN, pooled over many episodes into a replay memory.\n",
    "\n",
    "(what is an episode? probably describes a game from its start until the reaching of a terminal state. How do we know which experiences are valuable to keep?)\n",
    "\n",
    "(what does pooled mean? : you measure blood levels of substance X in males and females. You don't see statistical differences between the two groups so you pool the data together, ignoring the sex of the experimental subject.\n",
    "Whether it is statistically correct to do so depends very much on the specific case.\n",
    "In our exemple it means that we put the samples obtained in different episodes in the same data-set D, without discriminating between them.)\n",
    "\n",
    "During the inner loop of the algorithm, we apply Q-learning updates, or mini batch updates, to samples of experience, e ∼ D, drawn at random from the pool of stored samples.\n",
    "\n",
    "After performing experience replay, the agent selects and executes an action according to an epsilon-greedy policy (epsilon greedy policy? means GLIE? YES OK). \n",
    "\n",
    "Histories = inputs of the neural network/deep Q learning before being processed by the function φ. Must be always of same size. Is obtained with a function φ. Is equal to the last four frames and the last four actions. \n",
    "\n",
    "In practice, our algorithm only stores the last N experience tuples in the replay memory, and samples uniformly at random from D when performing updates. This approach is in some respects limited since the memory buffer does not differentiate important transitions and always overwrites with recent transitions due to the ﬁnite memory size N. Similarly, the uniform sampling gives equal importance to all transitions in the replay memory. A more sophisticated sampling strategy might emphasize transitions from which we can learn the most, similar to prioritized sweeping.\n",
    "\n",
    "NEXT QUESTION TO ASK OURSELVES : \n",
    "Which are the transitions from whom we can learn the most?\n",
    "What is prioritized sweeping? (might help us answer the last question)\n",
    "\n",
    "**************************\n",
    "Pre processing and Model Architecture :\n",
    "\n",
    "Working directly with raw Atari frames, which are 210×160 pixel images with a 128 color palette, can be computationally demanding, so we apply a basic preprocessing step aimed at reducing the input dimensionality. The raw frames are preprocessed by ﬁrst converting their RGB representation to gray-scal eand down-sampling it to a 110×84 image \n",
    "(ie less colors, less pixels). \n",
    "\n",
    "For the experiments in this paper, the function φ from algorithm 1 applies the preprocessing to the last 4 frames of a history and stacks them to produce the input to the Q-function (the last four frames! Which means: s_t= (x_t-4,a_t-4,x_t-3,a_t-3,x_t-2,a_t-2,x_t-1,a_t-1) et s_t=(x_t-3,a_t-3,x_t-2,a_t-2,x_t-1,a_t-1,x_t,a_t) (reutilise trois des screen utilises dans la sequence precedente!!)\n",
    "\n",
    "***************\n",
    "\n",
    "We now describe the exact architecture used for all seven Atari games. The input to the neural network consists is an 84×84×4 image produced by φ. The ﬁrst hidden layer convolves 16 8×8 ﬁlters with stride 4 with the input image and applies a rectiﬁer nonlinearity [10, 18]. The second hidden layer convolves 32 4×4 ﬁlters with stride 2, again followed by a rectiﬁer nonlinearity. The ﬁnal hidden layer is fully-connected and consists of 256 rectiﬁer units. The output layer is a fully connected linear layer with a single output for each validaction. The number of validactions varied between 4 and 18 on the games we considered. We refer to convolutional networks trained with our approach as Deep Q-Networks (DQN).\n",
    "In these experiments, we used the RMSProp algorithm with minibatches of size 32. The behavior policy during training was epsilon-greedy with epsilon annealed linearly from 1 to 0.1 over the ﬁrst million frames, and ﬁxed at 0.1 thereafter. We trained for a total of 10 million frames and used a replay memory of one million most recent frames.\n",
    "\n",
    "Following previous approaches to playing Atari games, we also use a simple frame-skipping technique [3]. More precisely, the agent sees and selects actions on every kth frame instead of every frame, and its lastaction is repeated on skipped frames. Sincerunning the emulator forward for one step requires much less computation than having the agent select an action, this technique allows the agent to play roughly k times more games without signiﬁcantly increasing the runtime. We use k = 4 for all games except Space Invaders where we noticed that using k = 4 makes the lasers invisible because of the period at which they blink. We used k = 3 to make the lasers visible and this change was the only difference in hyperparameter values between any of the games.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing : size of our image :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 512, 3)\n"
     ]
    }
   ],
   "source": [
    "game = FlappyBird(graphics=\"fixed\") # use \"fancy\" for full background, random bird color and random pipe color, use \"fixed\" (default) for black background and constant bird and pipe colors.\n",
    "p = PLE(game, fps=30, frame_skip=1, num_steps=1, force_fps=False, display_screen=True)\n",
    "# Note: if you want to see you agent act in real time, set force_fps to False. But don't use this setting for learning, just for display purposes.\n",
    "\n",
    "p.init()\n",
    "p.reset_game()\n",
    "screen = p.getScreenRGB()\n",
    "print(screen.shape)\n",
    "#?screen # is an array\n",
    "\n",
    "# first thing I want to do is crop the ground out of the picture because is useless.\n",
    "# Second thing I wanna do is convert to gray scale.\n",
    "# To do that, understand what the teacher did below and apply it to your case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "def process_screen(x):\n",
    "    return 256*resize(rgb2gray(x), (110,84))[17:101,:]\n",
    "\n",
    "y=process_screen(x)\n",
    "plt.imshow(y, cmap=\"gray\")\n",
    "plt.show()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stack the 4 last frames\n",
    "z = np.stack([y,y,y,y],axis=-1)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the next step, create our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "dqn = Sequential()\n",
    "#1st layer\n",
    "dqn.add(Conv2D(filters=16, kernel_size=(8,8), strides=4, activation=\"relu\", input_shape=(84,84,4)))\n",
    "#2nd layer\n",
    "dqn.add(Conv2D(filters=32, kernel_size=(4,4), strides=2, activation=\"relu\"))\n",
    "dqn.add(Flatten())\n",
    "#3rd layer\n",
    "dqn.add(Dense(units=256, activation=\"relu\"))\n",
    "#output layer\n",
    "dqn.add(Dense(units=4, activation=\"linear\"))\n",
    "\n",
    "dqn.compile(optimizer=\"rmsprop\", loss=\"mean_squared_error\")\n",
    "\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(dqn, to_file=\"images/dqn_keras.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#You can change the rewards in p=PLE(reward_values={reward..})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from ple.games.flappybird import FlappyBird\n",
    "from ple import PLE\n",
    "import numpy as np\n",
    "from FlappyAgent import FlappyPolicy\n",
    "\n",
    "game = FlappyBird(graphics=\"fixed\") # use \"fancy\" for full background, random bird color and random pipe color, use \"fixed\" (default) for black background and constant bird and pipe colors.\n",
    "p = PLE(game, fps=30, frame_skip=1, num_steps=1, force_fps=False, display_screen=True)\n",
    "# Note: if you want to see you agent act in real time, set force_fps to False. But don't use this setting for learning, just for display purposes.\n",
    "\n",
    "p.init()\n",
    "reward = 0.0\n",
    "\n",
    "nb_games = 100\n",
    "cumulated = np.zeros((nb_games))\n",
    "\n",
    "for i in range(nb_games):\n",
    "    p.reset_game()\n",
    "    \n",
    "    while(not p.game_over()):\n",
    "        state = game.getGameState()\n",
    "        screen = p.getScreenRGB()\n",
    "        action=FlappyPolicy(state, screen) ### Your job is to define this function.\n",
    "        \n",
    "        reward = p.act(action)\n",
    "        # ajouter un scaling des rewards\n",
    "        cumulated[i] = cumulated[i] + reward\n",
    "        # rt\n",
    "\n",
    "average_score = np.mean(cumulated)\n",
    "max_score = np.max(cumulated)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
